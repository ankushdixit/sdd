"""Unit tests for learning_curator module.

This module tests learning curator functionality including bug fixes for:
- Bug 1: Multi-line LEARNING statement extraction
- Bug 2: File type filtering and content validation
- Bug 3: Standardized metadata structure
- Bug #21: Test directory exclusion and code artifact rejection
"""

import re
from datetime import datetime
from unittest.mock import Mock, patch

import pytest

from sdd.learning.curator import LEARNING_SCHEMA, LearningsCurator


@pytest.fixture
def curator():
    """Create LearningsCurator instance for testing.

    Returns:
        LearningsCurator: Fresh curator instance.
    """
    return LearningsCurator()


@pytest.fixture
def temp_project(tmp_path):
    """Create temporary project structure for testing.

    Args:
        tmp_path: Pytest tmp_path fixture.

    Returns:
        tuple: (project_root Path, curator instance)
    """
    project_root = tmp_path / "test_project"
    project_root.mkdir()
    curator = LearningsCurator(project_root)
    return project_root, curator


@pytest.fixture
def learning_pattern():
    """Provide regex pattern for LEARNING extraction.

    Returns:
        str: Compiled regex pattern for multi-line LEARNING statements.
    """
    return r"LEARNING:\s*([\s\S]+?)(?=\n\n|\Z)"


class TestMultiLineLearningExtraction:
    """Test suite for Bug 1 fix: Multi-line LEARNING statement extraction from git commits."""

    def test_extract_single_line_learning(self, learning_pattern):
        """Test that single-line LEARNING statement is correctly extracted."""
        # Arrange
        commit_message = "feat: Add new feature\n\nLEARNING: This is a single line learning."

        # Act
        match = re.search(learning_pattern, commit_message)

        # Assert
        assert match is not None
        learning_text = match.group(1).strip()
        assert learning_text == "This is a single line learning."

    def test_extract_multi_line_learning_basic(self, learning_pattern):
        """Test that multi-line LEARNING statement spanning 2-3 lines is fully extracted."""
        # Arrange
        commit_message = """feat: Add authentication

LEARNING: This is a multi-line learning that spans
several lines to provide comprehensive context about
the implementation.

Some other text."""

        # Act
        match = re.search(learning_pattern, commit_message)

        # Assert
        assert match is not None
        learning_text = match.group(1).strip()
        assert "This is a multi-line learning that spans" in learning_text
        assert "several lines to provide comprehensive context" in learning_text
        assert "the implementation." in learning_text

    def test_extract_multi_line_learning_with_indentation(self, learning_pattern):
        """Test that multi-line LEARNING with indented continuation lines is extracted."""
        # Arrange
        commit_message = """fix: Bug fix

LEARNING: The .gitignore patterns are added programmatically in
  ensure_gitignore_entries() function, not from a template file.
  This allows dynamic checking of which patterns already exist.

Next paragraph."""

        # Act
        match = re.search(learning_pattern, commit_message)

        # Assert
        assert match is not None
        learning_text = match.group(1).strip()
        assert "ensure_gitignore_entries() function" in learning_text
        assert "This allows dynamic checking" in learning_text

    def test_extract_learning_followed_by_double_newline(self, learning_pattern):
        """Test that LEARNING statement stops at double newline separator."""
        # Arrange
        commit_message = """feat: New feature

LEARNING: Important learning here that should be captured.


Next section starts here."""

        # Act
        match = re.search(learning_pattern, commit_message)

        # Assert
        assert match is not None
        learning_text = match.group(1).strip()
        assert learning_text == "Important learning here that should be captured."
        assert "Next section" not in learning_text

    def test_extract_learning_with_wrapped_lines(self, learning_pattern):
        """Test that LEARNING captures wrapped continuation lines without blank lines."""
        # Arrange
        commit_message = """feat: Feature

LEARNING: This learning continues across multiple lines
without blank lines between them because it is describing
a single cohesive concept that needs detailed explanation.

Next paragraph starts here."""

        # Act
        match = re.search(learning_pattern, commit_message)

        # Assert
        assert match is not None
        learning_text = match.group(1).strip()
        assert "continues across multiple lines" in learning_text
        assert "without blank lines between them" in learning_text
        assert "detailed explanation" in learning_text
        assert "Next paragraph" not in learning_text


class TestFileTypeFiltering:
    """Test suite for Bug 2 fix: File type filtering to skip documentation."""

    def test_code_extensions_are_defined(self):
        """Test that code file extensions set is properly defined."""
        # Arrange & Act
        code_extensions = {".py", ".js", ".ts", ".jsx", ".tsx", ".go", ".rs"}

        # Assert
        assert len(code_extensions) > 0

    def test_doc_extensions_are_defined(self):
        """Test that documentation file extensions set is properly defined."""
        # Arrange & Act
        doc_extensions = {".md", ".txt", ".rst"}

        # Assert
        assert len(doc_extensions) > 0

    def test_skips_markdown_files(self, temp_project):
        """Test that markdown files are skipped during learning extraction."""
        # Arrange
        project_root, curator = temp_project
        code_file = project_root / "test.py"
        doc_file = project_root / "README.md"

        code_file.write_text("# LEARNING: Valid code learning with enough words here")
        doc_file.write_text("# LEARNING: <your learning here>")

        # Act
        learnings = curator.extract_from_code_comments(changed_files=[code_file, doc_file])

        # Assert
        assert any(
            "Valid code learning with enough words" in learning.get("content", "")
            for learning in learnings
        )
        assert not any(
            "your learning here" in learning.get("content", "") for learning in learnings
        )

    def test_skips_template_directories(self, temp_project):
        """Test that templates directory is skipped during learning extraction."""
        # Arrange
        project_root, curator = temp_project
        code_file = project_root / "test.py"
        template_dir = project_root / "templates"
        template_dir.mkdir()
        template_file = template_dir / "example.py"

        code_file.write_text("# LEARNING: Valid code learning from main file with words")
        template_file.write_text("# LEARNING: Example learning from template directory here")

        # Act
        learnings = curator.extract_from_code_comments(changed_files=[code_file, template_file])

        # Assert
        assert any(
            "Valid code learning from main file" in learning.get("content", "")
            for learning in learnings
        )
        assert not any(
            "Example learning from template" in learning.get("content", "")
            for learning in learnings
        )


class TestContentValidation:
    """Test suite for Bug 2 fix: Content validation to skip placeholders."""

    def test_valid_learning_passes_validation(self, curator):
        """Test that valid learning with 5+ words passes validation."""
        # Arrange
        valid_learning = "This is a valid learning with enough words"

        # Act & Assert
        assert curator.is_valid_learning(valid_learning)

    def test_rejects_angle_bracket_placeholders(self, curator):
        """Test that placeholders with angle brackets are rejected."""
        # Arrange
        placeholder = "<your learning here>"
        partial_placeholder = "This has <placeholder> text in it"

        # Act & Assert
        assert not curator.is_valid_learning(placeholder)
        assert not curator.is_valid_learning(partial_placeholder)

    def test_rejects_known_placeholder_text(self, curator):
        """Test that known placeholder text patterns are rejected."""
        # Arrange
        placeholders = ["your learning here", "example learning", "todo", "tbd", "placeholder"]

        # Act & Assert
        for placeholder in placeholders:
            assert not curator.is_valid_learning(placeholder)
            assert not curator.is_valid_learning(placeholder.upper())

    def test_rejects_content_with_fewer_than_five_words(self, curator):
        """Test that content with less than 5 words is rejected."""
        # Arrange
        short_content = "Too short here"  # 3 words
        exactly_five = "This has exactly five words"  # 5 words

        # Act & Assert
        assert not curator.is_valid_learning(short_content)
        assert curator.is_valid_learning(exactly_five)

    def test_rejects_empty_or_none_content(self, curator):
        """Test that empty string or None content is rejected."""
        # Act & Assert
        assert not curator.is_valid_learning("")
        assert not curator.is_valid_learning(None)

    def test_rejects_list_fragment_markers(self, curator):
        """Test that content with list markers is rejected as incomplete fragments."""
        # Arrange
        hyphen_list = "annotations\n- Code comments with enough words"
        asterisk_list = "Some content\n* List item with enough words"
        bullet_list = "Fragment text\nâ€¢ Bullet point with enough words"
        valid_multiline = (
            "This is a valid multi-line learning\nthat spans multiple lines without list markers"
        )

        # Act & Assert
        assert not curator.is_valid_learning(hyphen_list)
        assert not curator.is_valid_learning(asterisk_list)
        assert not curator.is_valid_learning(bullet_list)
        assert curator.is_valid_learning(valid_multiline)


class TestStandardizedEntryCreation:
    """Test suite for Bug 3 fix: Standardized learning entry creation."""

    def test_creates_entry_with_all_required_fields(self, curator):
        """Test that create_learning_entry includes all required fields."""
        # Act
        entry = curator.create_learning_entry(
            content="Test learning with enough words here",
            source="git_commit",
            session_id="session_001",
            context="Commit abc123",
        )

        # Assert
        assert "content" in entry
        assert "learned_in" in entry
        assert "source" in entry
        assert "context" in entry
        assert "timestamp" in entry
        assert "id" in entry

    def test_entry_values_match_input_parameters(self, curator):
        """Test that created entry values match the provided input parameters."""
        # Arrange
        content = "Test learning content with more words"
        source = "git_commit"
        session_id = "session_002"
        context = "Commit xyz789"

        # Act
        entry = curator.create_learning_entry(
            content=content, source=source, session_id=session_id, context=context
        )

        # Assert
        assert entry["content"] == content
        assert entry["learned_in"] == session_id
        assert entry["source"] == source
        assert entry["context"] == context

    def test_optional_fields_receive_default_values(self, curator):
        """Test that optional fields are populated with appropriate default values."""
        # Act
        entry = curator.create_learning_entry(
            content="Test learning with sufficient words present", source="git_commit"
        )

        # Assert
        assert entry["learned_in"] == "unknown"
        assert entry["context"] == "No context provided"
        assert entry["timestamp"] is not None
        assert entry["id"] is not None

    def test_generates_consistent_id_for_same_content(self, curator):
        """Test that identical content generates the same ID consistently."""
        # Arrange
        content = "Test learning for ID consistency check here"

        # Act
        entry1 = curator.create_learning_entry(content=content, source="test")
        entry2 = curator.create_learning_entry(content=content, source="test")

        # Assert
        assert entry1["id"] == entry2["id"]

    def test_git_extracted_learnings_have_both_fields(self, curator):
        """Test that git-extracted learnings include both learned_in and context fields."""
        # Arrange & Act
        with patch("subprocess.run") as mock_run:
            mock_run.return_value = Mock(
                returncode=0,
                stdout="abc123|||feat: Test\n\nLEARNING: Git commit learning with enough words",
            )
            learnings = curator.extract_from_git_commits(session_id="session_003")

            # Assert
            if learnings:
                learning = learnings[0]
                assert "learned_in" in learning
                assert "context" in learning
                assert learning["learned_in"] == "session_003"

    def test_temp_file_entry_has_both_fields(self, curator):
        """Test that temp-file sourced learnings include both learned_in and context."""
        # Act
        entry = curator.create_learning_entry(
            content="Temp file learning with many words here",
            source="temp_file",
            session_id="session_004",
            context="Temp file: .session/temp_learnings.txt",
        )

        # Assert
        assert "learned_in" in entry
        assert "context" in entry
        assert entry["learned_in"] == "session_004"
        assert entry["context"] == "Temp file: .session/temp_learnings.txt"


class TestJSONSchemaValidation:
    """Test suite for Bug 3 fix: JSON schema validation for learning entries."""

    def test_valid_entry_passes_schema_validation(self, curator):
        """Test that a properly formatted learning entry passes JSON schema validation."""
        # Arrange
        valid_entry = {
            "content": "This is a valid learning with more than ten characters",
            "learned_in": "session_001",
            "source": "git_commit",
            "context": "Commit abc123",
            "timestamp": datetime.now().isoformat(),
            "id": "test123",
        }

        # Act & Assert
        assert curator.validate_learning(valid_entry)

    def test_missing_required_field_fails_validation(self, curator):
        """Test that entry missing required field fails schema validation."""
        # Arrange
        invalid_entry = {
            "content": "Valid content with enough characters",
            "learned_in": "session_001",
            "source": "git_commit",
            "timestamp": datetime.now().isoformat(),
            "id": "test123",
            # Missing required "context" field
        }

        # Act & Assert
        assert not curator.validate_learning(invalid_entry)

    def test_invalid_source_type_fails_validation(self, curator):
        """Test that entry with invalid source enum value fails validation."""
        # Arrange
        invalid_entry = {
            "content": "Valid content with enough characters",
            "learned_in": "session_001",
            "source": "invalid_source",  # Not in enum
            "context": "Test context",
            "timestamp": datetime.now().isoformat(),
            "id": "test123",
        }

        # Act & Assert
        assert not curator.validate_learning(invalid_entry)

    def test_short_content_fails_min_length_validation(self, curator):
        """Test that content shorter than minLength constraint fails validation."""
        # Arrange
        invalid_entry = {
            "content": "Short",  # Less than 10 characters
            "learned_in": "session_001",
            "source": "git_commit",
            "context": "Test context",
            "timestamp": datetime.now().isoformat(),
            "id": "test123",
        }

        # Act & Assert
        assert not curator.validate_learning(invalid_entry)

    def test_schema_defines_all_required_fields(self):
        """Test that LEARNING_SCHEMA includes all required field definitions."""
        # Arrange
        required_fields = ["content", "learned_in", "source", "context", "timestamp", "id"]

        # Act & Assert
        assert set(LEARNING_SCHEMA["required"]) == set(required_fields)

    def test_schema_defines_valid_source_enum_values(self):
        """Test that schema defines complete set of valid source types."""
        # Act
        source_enum = LEARNING_SCHEMA["properties"]["source"]["enum"]

        # Assert
        assert "git_commit" in source_enum
        assert "temp_file" in source_enum
        assert "inline_comment" in source_enum
        assert "session_summary" in source_enum


class TestEdgeCases:
    """Test suite for edge cases in learning extraction."""

    def test_learning_with_special_characters_and_emojis(self, curator):
        """Test that learnings with special characters and emojis are handled correctly."""
        # Arrange
        special_content = "Learning with special chars: @#$% and Ã©mojis ðŸŽ‰ works fine"

        # Act & Assert
        assert curator.is_valid_learning(special_content)
        entry = curator.create_learning_entry(content=special_content, source="git_commit")
        assert curator.validate_learning(entry)

    def test_very_long_learning_content(self, curator):
        """Test that very long learning content (100+ words) is handled correctly."""
        # Arrange
        long_content = " ".join(["word"] * 150)  # 150 words

        # Act & Assert
        assert curator.is_valid_learning(long_content)
        entry = curator.create_learning_entry(content=long_content, source="git_commit")
        assert curator.validate_learning(entry)

    def test_empty_learning_statement_is_rejected(self, curator):
        """Test that empty LEARNING statement is properly rejected."""
        # Arrange
        empty_content = ""

        # Act & Assert
        assert not curator.is_valid_learning(empty_content)

    def test_multiple_learnings_in_single_commit(self):
        """Test extraction of multiple LEARNING statements from single commit message."""
        # Arrange
        commit_message = """feat: Multiple learnings

LEARNING: First learning with enough words to be valid

LEARNING: Second learning also with sufficient word count"""
        learning_pattern = r"LEARNING:\s*([\s\S]+?)(?=\n(?![ \t])|\n\n|\Z)"

        # Act
        matches = list(re.finditer(learning_pattern, commit_message))

        # Assert
        assert len(matches) == 2


class TestTestDirectoryExclusion:
    """Test suite for Bug #21 Fix 1: Test directories are excluded from learning extraction."""

    def test_excludes_tests_directory(self, temp_project):
        """Test that tests/ directory is excluded from learning extraction."""
        # Arrange
        project_root, curator = temp_project
        tests_dir = project_root / "tests"
        tests_dir.mkdir()
        src_dir = project_root / "src"
        src_dir.mkdir()

        tests_file = tests_dir / "test_example.py"
        prod_file = src_dir / "main.py"

        tests_file.write_text("# LEARNING: Test data from tests directory with words")
        prod_file.write_text("# LEARNING: Production learning from src directory here")

        # Act
        learnings = curator.extract_from_code_comments(changed_files=[tests_file, prod_file])

        # Assert
        assert any(
            "Production learning from src" in entry.get("content", "") for entry in learnings
        )
        assert not any("Test data from tests" in entry.get("content", "") for entry in learnings)

    def test_excludes_test_directory(self, temp_project):
        """Test that test/ directory (singular) is excluded from learning extraction."""
        # Arrange
        project_root, curator = temp_project
        test_dir = project_root / "test"
        test_dir.mkdir()
        src_dir = project_root / "src"
        src_dir.mkdir()

        test_file = test_dir / "test_example.py"
        prod_file = src_dir / "main.py"

        test_file.write_text("# LEARNING: Test data from test directory with words")
        prod_file.write_text("# LEARNING: Production learning from src directory here")

        # Act
        learnings = curator.extract_from_code_comments(changed_files=[test_file, prod_file])

        # Assert
        assert any(
            "Production learning from src" in entry.get("content", "") for entry in learnings
        )
        assert not any("Test data from test" in entry.get("content", "") for entry in learnings)

    def test_excludes_jest_tests_directory(self, temp_project):
        """Test that __tests__/ directory (Jest convention) is excluded from extraction."""
        # Arrange
        project_root, curator = temp_project
        jest_dir = project_root / "__tests__"
        jest_dir.mkdir()
        src_dir = project_root / "src"
        src_dir.mkdir()

        jest_file = jest_dir / "example.test.js"
        prod_file = src_dir / "main.py"

        jest_file.write_text("// LEARNING: Test data from jest tests directory here")
        prod_file.write_text("# LEARNING: Production learning from src directory here")

        # Act
        learnings = curator.extract_from_code_comments(changed_files=[jest_file, prod_file])

        # Assert
        assert any(
            "Production learning from src" in entry.get("content", "") for entry in learnings
        )
        assert not any("Test data from jest" in entry.get("content", "") for entry in learnings)

    def test_excludes_spec_directory(self, temp_project):
        """Test that spec/ directory (RSpec convention) is excluded from extraction."""
        # Arrange
        project_root, curator = temp_project
        spec_dir = project_root / "spec"
        spec_dir.mkdir()
        src_dir = project_root / "src"
        src_dir.mkdir()

        spec_file = spec_dir / "example_spec.rb"
        prod_file = src_dir / "main.py"

        spec_file.write_text("# LEARNING: Test data from spec directory with words")
        prod_file.write_text("# LEARNING: Production learning from src directory here")

        # Act
        learnings = curator.extract_from_code_comments(changed_files=[spec_file, prod_file])

        # Assert
        assert any(
            "Production learning from src" in entry.get("content", "") for entry in learnings
        )
        assert not any("Test data from spec" in entry.get("content", "") for entry in learnings)


class TestCodeArtifactValidation:
    """Test suite for Bug #21 Fix 2: Content validation rejects code artifacts."""

    def test_rejects_content_ending_with_quote_paren(self, curator):
        """Test rejection of content ending with quote-paren from string literals."""
        # Arrange
        artifact_content = 'Valid code learning with enough words here")'

        # Act & Assert
        assert not curator.is_valid_learning(artifact_content)

    def test_rejects_content_with_escaped_quote(self, curator):
        """Test rejection of content containing escaped quotes."""
        # Arrange
        artifact_content = 'Learning with \\" escaped quote and words'

        # Act & Assert
        assert not curator.is_valid_learning(artifact_content)

    def test_rejects_content_with_visible_newline_escape(self, curator):
        """Test rejection of content with visible \\n escape sequences."""
        # Arrange
        artifact_content = "Learning with \\n visible newline and more words"

        # Act & Assert
        assert not curator.is_valid_learning(artifact_content)

    def test_rejects_content_with_backticks(self, curator):
        """Test rejection of content with backticks indicating code fragments."""
        # Arrange
        artifact_content = "Learning with `code` backticks and more words"

        # Act & Assert
        assert not curator.is_valid_learning(artifact_content)

    def test_rejects_content_ending_with_quote_semicolon(self, curator):
        """Test rejection of content ending with quote-semicolon pattern."""
        # Arrange
        artifact_content = 'Valid learning with enough words here";'

        # Act & Assert
        assert not curator.is_valid_learning(artifact_content)

    def test_rejects_content_ending_with_single_quote_paren(self, curator):
        """Test rejection of content ending with single-quote-paren pattern."""
        # Arrange
        artifact_content = "Valid learning with enough words here')"

        # Act & Assert
        assert not curator.is_valid_learning(artifact_content)

    def test_accepts_clean_valid_learning(self, curator):
        """Test that clean valid learnings without code artifacts are accepted."""
        # Arrange
        clean_content = "This is a clean learning with no code artifacts"

        # Act & Assert
        assert curator.is_valid_learning(clean_content)

    def test_accepts_learning_with_normal_punctuation(self, curator):
        """Test that learnings with normal punctuation marks are accepted."""
        # Arrange
        normal_content = "Learning with normal punctuation: commas, periods, and more."

        # Act & Assert
        assert curator.is_valid_learning(normal_content)


class TestRegexPatternStrictness:
    """Test suite for Bug #21 Fix 3: Regex pattern only matches actual comment lines."""

    def test_extracts_actual_comment_line(self, temp_project):
        """Test that actual # LEARNING comments are successfully extracted."""
        # Arrange
        project_root, curator = temp_project
        src_dir = project_root / "src"
        src_dir.mkdir()
        test_file = src_dir / "test.py"

        code = """# LEARNING: This is an actual comment with words
def foo():
    pass
"""
        test_file.write_text(code)

        # Act
        learnings = curator.extract_from_code_comments(changed_files=[test_file])

        # Assert
        assert len(learnings) == 1
        assert "actual comment" in learnings[0]["content"]

    def test_extracts_indented_comment_line(self, temp_project):
        """Test that indented # LEARNING comments are extracted correctly."""
        # Arrange
        project_root, curator = temp_project
        src_dir = project_root / "src"
        src_dir.mkdir()
        test_file = src_dir / "test.py"

        code = """def foo():
    # LEARNING: Indented learning comment with enough words here
    pass
"""
        test_file.write_text(code)

        # Act
        learnings = curator.extract_from_code_comments(changed_files=[test_file])

        # Assert
        assert len(learnings) == 1
        assert "Indented learning" in learnings[0]["content"]

    def test_does_not_extract_string_literal(self, temp_project):
        """Test that string literals containing LEARNING pattern are NOT extracted."""
        # Arrange
        project_root, curator = temp_project
        src_dir = project_root / "src"
        src_dir.mkdir()
        test_file = src_dir / "test.py"

        code = """def test_example():
    test_data = "# LEARNING: This is test data not comment"
    file.write_text("# LEARNING: Another test string with enough words here")
"""
        test_file.write_text(code)

        # Act
        learnings = curator.extract_from_code_comments(changed_files=[test_file])

        # Assert
        assert len(learnings) == 0

    def test_does_not_extract_learning_in_middle_of_line(self, temp_project):
        """Test that LEARNING pattern not at line start is NOT extracted."""
        # Arrange
        project_root, curator = temp_project
        src_dir = project_root / "src"
        src_dir.mkdir()
        test_file = src_dir / "test.py"

        code = """def foo():
    x = "some string # LEARNING: This should not be extracted ever"
"""
        test_file.write_text(code)

        # Act
        learnings = curator.extract_from_code_comments(changed_files=[test_file])

        # Assert
        assert len(learnings) == 0

    def test_extracts_multiple_actual_comments(self, temp_project):
        """Test that multiple actual # LEARNING comments are all extracted."""
        # Arrange
        project_root, curator = temp_project
        src_dir = project_root / "src"
        src_dir.mkdir()
        test_file = src_dir / "test.py"

        code = """# LEARNING: First learning comment with enough words here
def foo():
    # LEARNING: Second learning comment also with enough words
    pass

# LEARNING: Third learning comment at end with words
"""
        test_file.write_text(code)

        # Act
        learnings = curator.extract_from_code_comments(changed_files=[test_file])

        # Assert
        assert len(learnings) == 3
        assert any("First learning" in entry["content"] for entry in learnings)
        assert any("Second learning" in entry["content"] for entry in learnings)
        assert any("Third learning" in entry["content"] for entry in learnings)


class TestBug21Integration:
    """Integration test suite for Bug #21: All three fixes working together."""

    def test_full_bug_scenario_extracts_nothing(self, temp_project):
        """Test complete Bug #21 scenario: test file with string literals extracts nothing."""
        # Arrange
        project_root, curator = temp_project
        tests_dir = project_root / "tests"
        tests_dir.mkdir()
        test_file = tests_dir / "test_learning_curator_bug_fixes.py"

        test_content = """def test_example():
    test_data = "# LEARNING: This is test data"
    file.write_text("# LEARNING: Valid code learning with enough words here")
    assert test_data
"""
        test_file.write_text(test_content)

        # Act
        learnings = curator.extract_from_code_comments(changed_files=[test_file])

        # Assert
        assert len(learnings) == 0

    def test_production_code_comments_still_work(self, temp_project):
        """Test that production code with real # LEARNING comments works correctly."""
        # Arrange
        project_root, curator = temp_project
        src_dir = project_root / "src"
        src_dir.mkdir()
        prod_file = src_dir / "main.py"

        prod_content = """# LEARNING: Production learning comment with enough words

def important_function():
    # LEARNING: Another production learning with sufficient word count
    pass
"""
        prod_file.write_text(prod_content)

        # Act
        learnings = curator.extract_from_code_comments(changed_files=[prod_file])

        # Assert
        assert len(learnings) == 2
        assert any("Production learning comment" in entry["content"] for entry in learnings)
        assert any("Another production learning" in entry["content"] for entry in learnings)

    def test_mixed_test_and_production_files(self, temp_project):
        """Test mixed scenario with both test files and production files."""
        # Arrange
        project_root, curator = temp_project
        tests_dir = project_root / "tests"
        tests_dir.mkdir()
        src_dir = project_root / "src"
        src_dir.mkdir()

        test_file = tests_dir / "test_example.py"
        test_file.write_text('file.write_text("# LEARNING: Test data with enough words here")')

        prod_file = src_dir / "main.py"
        prod_file.write_text("# LEARNING: Real production learning with enough words")

        # Act
        learnings = curator.extract_from_code_comments(changed_files=[test_file, prod_file])

        # Assert
        assert len(learnings) == 1
        assert "Real production learning" in learnings[0]["content"]

    def test_no_garbage_entries_with_code_artifacts(self, temp_project):
        """Test that no garbage entries containing code artifacts are created."""
        # Arrange
        project_root, curator = temp_project
        tests_dir = project_root / "tests"
        tests_dir.mkdir()
        src_dir = project_root / "src"
        src_dir.mkdir()

        test_file = tests_dir / "test.py"
        test_file.write_text('x = "# LEARNING: annotations\\n- Code comments with words")')

        # Act
        learnings = curator.extract_from_code_comments(changed_files=[test_file])

        # Assert
        assert len(learnings) == 0

        # Even if directory check is bypassed, content validation should reject
        prod_file = src_dir / "broken.py"
        prod_file.write_text('x = "# LEARNING: Valid code learning with enough words here")')

        learnings = curator.extract_from_code_comments(changed_files=[prod_file])
        assert len(learnings) == 0


# ============================================================================
# Additional Comprehensive Coverage Tests
# ============================================================================


class TestCountingAndMetadataOperations:
    """Test learning counting and metadata operations."""

    def test_count_all_learnings_empty_categories(self, curator):
        """Test counting with empty categories returns 0."""
        # Arrange
        learnings = {"categories": {}}

        # Act
        count = curator._count_all_learnings(learnings)

        # Assert
        assert count == 0

    def test_count_all_learnings_multiple_categories(self, curator):
        """Test counting across multiple categories."""
        # Arrange
        learnings = {
            "categories": {
                "best_practices": [{"id": "1"}, {"id": "2"}],
                "gotchas": [{"id": "3"}],
                "technical_debt": [{"id": "4"}, {"id": "5"}, {"id": "6"}],
            }
        }

        # Act
        count = curator._count_all_learnings(learnings)

        # Assert
        assert count == 6

    def test_count_includes_archived_learnings(self, curator):
        """Test that count includes archived learnings."""
        # Arrange
        learnings = {
            "categories": {"best_practices": [{"id": "1"}]},
            "archived": [{"id": "2"}, {"id": "3"}],
        }

        # Act
        count = curator._count_all_learnings(learnings)

        # Assert
        assert count == 3

    def test_update_total_learnings_creates_metadata(self, curator):
        """Test that update creates metadata if missing."""
        # Arrange
        learnings = {"categories": {"best_practices": [{"id": "1"}, {"id": "2"}]}}

        # Act
        curator._update_total_learnings(learnings)

        # Assert
        assert "metadata" in learnings
        assert learnings["metadata"]["total_learnings"] == 2

    def test_update_total_learnings_updates_existing(self, curator):
        """Test that update modifies existing metadata."""
        # Arrange
        learnings = {
            "metadata": {"total_learnings": 0},
            "categories": {"best_practices": [{"id": "1"}]},
        }

        # Act
        curator._update_total_learnings(learnings)

        # Assert
        assert learnings["metadata"]["total_learnings"] == 1


class TestKeywordScoringAndAutoCategorization:
    """Test keyword scoring and auto-categorization logic."""

    def test_keyword_score_no_matches(self, curator):
        """Test keyword scoring with no matches."""
        # Arrange
        text = "This text has no relevant keywords"
        keywords = ["database", "connection", "query"]

        # Act
        score = curator._keyword_score(text, keywords)

        # Assert
        assert score == 0

    def test_keyword_score_single_match(self, curator):
        """Test keyword scoring with single match."""
        # Arrange
        text = "The database connection is important"
        keywords = ["database", "query", "transaction"]

        # Act
        score = curator._keyword_score(text, keywords)

        # Assert
        assert score == 1

    def test_keyword_score_multiple_matches(self, curator):
        """Test keyword scoring with multiple keyword matches."""
        # Arrange
        text = "The database connection requires a query transaction"
        keywords = ["database", "connection", "query", "transaction"]

        # Act
        score = curator._keyword_score(text, keywords)

        # Assert
        assert score == 4

    def test_auto_categorize_architecture_pattern(self, curator):
        """Test auto-categorization for architecture patterns."""
        # Arrange
        learning = {"content": "The layered architecture design pattern separates concerns"}

        # Act
        category = curator._auto_categorize_learning(learning)

        # Assert
        assert category == "architecture_patterns"

    def test_auto_categorize_gotcha(self, curator):
        """Test auto-categorization for gotchas."""
        # Arrange
        learning = {"content": "This is a common pitfall error when handling concurrency"}

        # Act
        category = curator._auto_categorize_learning(learning)

        # Assert
        assert category == "gotchas"

    def test_auto_categorize_best_practice(self, curator):
        """Test auto-categorization for best practices."""
        # Arrange
        learning = {"content": "Best practice recommends always validating user input"}

        # Act
        category = curator._auto_categorize_learning(learning)

        # Assert
        assert category == "best_practices"

    def test_auto_categorize_technical_debt(self, curator):
        """Test auto-categorization for technical debt."""
        # Arrange
        learning = {"content": "Need to refactor this legacy code and cleanup workarounds"}

        # Act
        category = curator._auto_categorize_learning(learning)

        # Assert
        assert category == "technical_debt"

    def test_auto_categorize_performance_insight(self, curator):
        """Test auto-categorization for performance insights."""
        # Arrange
        learning = {"content": "Performance optimization reduced memory usage significantly"}

        # Act
        category = curator._auto_categorize_learning(learning)

        # Assert
        assert category == "performance_insights"

    def test_auto_categorize_respects_suggested_type(self, curator):
        """Test that suggested_type is respected over keyword analysis."""
        # Arrange
        learning = {"content": "Some content about performance", "suggested_type": "best_practice"}

        # Act
        category = curator._auto_categorize_learning(learning)

        # Assert
        assert category == "best_practices"

    def test_auto_categorize_defaults_to_best_practices(self, curator):
        """Test default categorization when no keywords match."""
        # Arrange
        learning = {"content": "Generic content without specific category keywords"}

        # Act
        category = curator._auto_categorize_learning(learning)

        # Assert
        assert category == "best_practices"


class TestSimilarityDetection:
    """Test similarity detection for deduplication."""

    def test_are_similar_exact_match(self, curator):
        """Test that exact content matches are detected as similar."""
        # Arrange
        learning_a = {"content": "Use dependency injection for testability"}
        learning_b = {"content": "Use dependency injection for testability"}

        # Act
        result = curator._are_similar(learning_a, learning_b)

        # Assert
        assert result is True

    def test_are_similar_high_overlap(self, curator):
        """Test high word overlap is detected as similar."""
        # Arrange
        # Using very similar content to ensure high Jaccard/containment similarity
        learning_a = {"content": "dependency injection testability maintainability patterns"}
        learning_b = {"content": "dependency injection testability maintainability"}

        # Act
        result = curator._are_similar(learning_a, learning_b)

        # Assert
        assert result is True

    def test_are_similar_different_content(self, curator):
        """Test different content is not similar."""
        # Arrange
        learning_a = {"content": "Use dependency injection"}
        learning_b = {"content": "Configure logging system"}

        # Act
        result = curator._are_similar(learning_a, learning_b)

        # Assert
        assert result is False

    def test_are_similar_empty_content(self, curator):
        """Test empty content returns False."""
        # Arrange
        learning_a = {"content": ""}
        learning_b = {"content": "Some content"}

        # Act
        result = curator._are_similar(learning_a, learning_b)

        # Assert
        assert result is False

    def test_learning_exists_finds_duplicate(self, curator):
        """Test that existing similar learning is found."""
        # Arrange
        existing = [{"content": "Use dependency injection"}]
        new_learning = {"content": "Use dependency injection"}

        # Act
        result = curator._learning_exists(existing, new_learning)

        # Assert
        assert result is True

    def test_learning_exists_no_duplicate(self, curator):
        """Test that non-duplicate returns False."""
        # Arrange
        existing = [{"content": "Use dependency injection"}]
        new_learning = {"content": "Configure logging properly"}

        # Act
        result = curator._learning_exists(existing, new_learning)

        # Assert
        assert result is False


class TestMergingLearnings:
    """Test learning merging functionality."""

    def test_merge_learning_combines_applies_to(self, curator):
        """Test that merge combines applies_to lists."""
        # Arrange
        target = {"applies_to": ["module1", "module2"]}
        source = {"applies_to": ["module3"]}

        # Act
        curator._merge_learning(target, source)

        # Assert
        assert set(target["applies_to"]) == {"module1", "module2", "module3"}

    def test_merge_learning_adds_merged_from_tracking(self, curator):
        """Test that merge adds merged_from metadata."""
        # Arrange
        target = {}
        source = {"learned_in": "session_005"}

        # Act
        curator._merge_learning(target, source)

        # Assert
        assert "merged_from" in target
        assert "session_005" in target["merged_from"]

    def test_merge_similar_learnings_merges_duplicates(self, curator):
        """Test merging of duplicate learnings within categories."""
        # Arrange
        learnings = {
            "categories": {
                "best_practices": [
                    {"content": "Use dependency injection"},
                    {"content": "Use dependency injection"},
                ]
            }
        }

        # Act
        merged_count = curator._merge_similar_learnings(learnings)

        # Assert
        assert merged_count == 1
        assert len(learnings["categories"]["best_practices"]) == 1

    def test_merge_similar_learnings_no_duplicates(self, curator):
        """Test merge with no duplicates returns 0."""
        # Arrange
        learnings = {
            "categories": {
                "best_practices": [
                    {"content": "Use dependency injection"},
                    {"content": "Configure logging system"},
                ]
            }
        }

        # Act
        merged_count = curator._merge_similar_learnings(learnings)

        # Assert
        assert merged_count == 0


class TestArchivalFunctionality:
    """Test archival of old learnings."""

    def test_archive_old_learnings_archives_old_sessions(self, temp_project):
        """Test that old learnings are archived."""
        # Arrange
        project_root, curator = temp_project
        work_items_file = project_root / ".session" / "tracking" / "work_items.json"
        work_items_file.parent.mkdir(parents=True)

        import json

        work_items_data = {"work_items": {"WI-001": {"sessions": [100]}}}
        work_items_file.write_text(json.dumps(work_items_data))

        learnings = {
            "categories": {
                "best_practices": [{"content": "Old learning", "learned_in": "session_001"}]
            }
        }

        # Act
        count = curator._archive_old_learnings(learnings, max_age_sessions=50)

        # Assert
        assert count == 1
        assert len(learnings["categories"]["best_practices"]) == 0
        assert len(learnings.get("archived", [])) == 1

    def test_archive_old_learnings_keeps_recent(self, temp_project):
        """Test that recent learnings are not archived."""
        # Arrange
        project_root, curator = temp_project
        work_items_file = project_root / ".session" / "tracking" / "work_items.json"
        work_items_file.parent.mkdir(parents=True)

        import json

        work_items_data = {"work_items": {"WI-001": {"sessions": [10]}}}
        work_items_file.write_text(json.dumps(work_items_data))

        learnings = {
            "categories": {
                "best_practices": [{"content": "Recent learning", "learned_in": "session_009"}]
            }
        }

        # Act
        count = curator._archive_old_learnings(learnings, max_age_sessions=5)

        # Assert
        assert count == 0
        assert len(learnings["categories"]["best_practices"]) == 1

    def test_archive_adds_metadata(self, temp_project):
        """Test that archival adds metadata to archived learnings."""
        # Arrange
        project_root, curator = temp_project
        work_items_file = project_root / ".session" / "tracking" / "work_items.json"
        work_items_file.parent.mkdir(parents=True)

        import json

        work_items_data = {"work_items": {"WI-001": {"sessions": [100]}}}
        work_items_file.write_text(json.dumps(work_items_data))

        learnings = {
            "categories": {
                "best_practices": [{"content": "Old learning", "learned_in": "session_001"}]
            }
        }

        # Act
        curator._archive_old_learnings(learnings)

        # Assert
        archived = learnings.get("archived", [])[0]
        assert "archived_from" in archived
        assert "archived_at" in archived


class TestSessionNumberExtraction:
    """Test session number extraction from various formats."""

    def test_extract_session_number_standard_format(self, curator):
        """Test extracting from standard session_XXX format."""
        # Act
        result = curator._extract_session_number("session_015")

        # Assert
        assert result == 15

    def test_extract_session_number_numeric_only(self, curator):
        """Test extracting from numeric string."""
        # Act
        result = curator._extract_session_number("042")

        # Assert
        assert result == 42

    def test_extract_session_number_no_digits(self, curator):
        """Test extracting from non-numeric string returns 0."""
        # Act
        result = curator._extract_session_number("unknown")

        # Assert
        assert result == 0

    def test_get_current_session_number_from_work_items(self, temp_project):
        """Test getting current session from work items."""
        # Arrange
        project_root, curator = temp_project
        work_items_file = project_root / ".session" / "tracking" / "work_items.json"
        work_items_file.parent.mkdir(parents=True)

        import json

        work_items_data = {
            "work_items": {
                "WI-001": {"sessions": [1, 2, 3]},
                "WI-002": {"sessions": [4, 5]},
            }
        }
        work_items_file.write_text(json.dumps(work_items_data))

        # Act
        result = curator._get_current_session_number()

        # Assert
        assert result == 5

    def test_get_current_session_number_no_file(self, curator):
        """Test getting session number when file doesn't exist."""
        # Act
        result = curator._get_current_session_number()

        # Assert
        assert result == 0


class TestSearchFunctionality:
    """Test learning search functionality."""

    def test_search_learnings_finds_matches(self, temp_project, capsys):
        """Test search finds matches in content."""
        # Arrange
        project_root, curator = temp_project
        learnings_file = project_root / ".session" / "tracking" / "learnings.json"
        learnings_file.parent.mkdir(parents=True)

        import json

        learnings_data = {
            "categories": {"best_practices": [{"content": "Use dependency injection", "id": "1"}]}
        }
        learnings_file.write_text(json.dumps(learnings_data))

        # Act
        curator.search_learnings("dependency")

        # Assert
        captured = capsys.readouterr()
        assert "dependency injection" in captured.out

    def test_search_learnings_no_matches(self, temp_project, capsys):
        """Test search with no matches."""
        # Arrange
        project_root, curator = temp_project
        learnings_file = project_root / ".session" / "tracking" / "learnings.json"
        learnings_file.parent.mkdir(parents=True)

        import json

        learnings_data = {"categories": {"best_practices": []}}
        learnings_file.write_text(json.dumps(learnings_data))

        # Act
        curator.search_learnings("nonexistent")

        # Assert
        captured = capsys.readouterr()
        assert "No learnings found" in captured.out

    def test_search_learnings_finds_in_tags(self, temp_project, capsys):
        """Test search finds matches in tags."""
        # Arrange
        project_root, curator = temp_project
        learnings_file = project_root / ".session" / "tracking" / "learnings.json"
        learnings_file.parent.mkdir(parents=True)

        import json

        learnings_data = {
            "categories": {
                "best_practices": [
                    {"content": "Some learning", "tags": ["testing", "quality"], "id": "1"}
                ]
            }
        }
        learnings_file.write_text(json.dumps(learnings_data))

        # Act
        curator.search_learnings("testing")

        # Assert
        captured = capsys.readouterr()
        assert "Some learning" in captured.out


class TestShowLearnings:
    """Test show learnings with filters."""

    def test_show_learnings_filters_by_category(self, temp_project, capsys):
        """Test showing learnings filtered by category."""
        # Arrange
        project_root, curator = temp_project
        learnings_file = project_root / ".session" / "tracking" / "learnings.json"
        learnings_file.parent.mkdir(parents=True)

        import json

        learnings_data = {
            "categories": {
                "best_practices": [{"content": "Best practice", "id": "1"}],
                "gotchas": [{"content": "Gotcha", "id": "2"}],
            }
        }
        learnings_file.write_text(json.dumps(learnings_data))

        # Act
        curator.show_learnings(category="best_practices")

        # Assert
        captured = capsys.readouterr()
        assert "Best practice" in captured.out
        assert "Gotcha" not in captured.out

    def test_show_learnings_filters_by_session(self, temp_project, capsys):
        """Test showing learnings filtered by session."""
        # Arrange
        project_root, curator = temp_project
        learnings_file = project_root / ".session" / "tracking" / "learnings.json"
        learnings_file.parent.mkdir(parents=True)

        import json

        learnings_data = {
            "categories": {
                "best_practices": [
                    {"content": "Session 5", "learned_in": "session_005", "id": "1"},
                    {"content": "Session 10", "learned_in": "session_010", "id": "2"},
                ]
            }
        }
        learnings_file.write_text(json.dumps(learnings_data))

        # Act
        curator.show_learnings(session=5)

        # Assert
        captured = capsys.readouterr()
        assert "Session 5" in captured.out
        assert "Session 10" not in captured.out


class TestGetRelatedLearnings:
    """Test finding related learnings."""

    def test_get_related_learnings_finds_similar(self, temp_project):
        """Test that related learnings are found."""
        # Arrange
        project_root, curator = temp_project
        learnings_file = project_root / ".session" / "tracking" / "learnings.json"
        learnings_file.parent.mkdir(parents=True)

        import json

        # Using very similar content to ensure similarity detection
        learnings_data = {
            "categories": {
                "best_practices": [
                    {
                        "content": "dependency injection testability maintainability patterns",
                        "id": "1",
                    },
                    {"content": "dependency injection testability maintainability", "id": "2"},
                    {"content": "Configure logging properly", "id": "3"},
                ]
            }
        }
        learnings_file.write_text(json.dumps(learnings_data))

        # Act
        result = curator.get_related_learnings("1")

        # Assert
        assert len(result) >= 1

    def test_get_related_learnings_nonexistent_id(self, curator):
        """Test related learnings with nonexistent ID."""
        # Act
        result = curator.get_related_learnings("nonexistent")

        # Assert
        assert result == []


class TestStatisticsGeneration:
    """Test statistics generation."""

    def test_generate_statistics_counts_by_category(self, temp_project):
        """Test statistics counts by category."""
        # Arrange
        project_root, curator = temp_project
        learnings_file = project_root / ".session" / "tracking" / "learnings.json"
        learnings_file.parent.mkdir(parents=True)

        import json

        learnings_data = {
            "categories": {
                "best_practices": [{"id": "1"}, {"id": "2"}],
                "gotchas": [{"id": "3"}],
            }
        }
        learnings_file.write_text(json.dumps(learnings_data))

        # Act
        stats = curator.generate_statistics()

        # Assert
        assert stats["total"] == 3
        assert stats["by_category"]["best_practices"] == 2
        assert stats["by_category"]["gotchas"] == 1

    def test_generate_statistics_counts_by_tag(self, temp_project):
        """Test statistics counts by tag."""
        # Arrange
        project_root, curator = temp_project
        learnings_file = project_root / ".session" / "tracking" / "learnings.json"
        learnings_file.parent.mkdir(parents=True)

        import json

        learnings_data = {
            "categories": {
                "best_practices": [
                    {"id": "1", "tags": ["testing", "quality"]},
                    {"id": "2", "tags": ["testing"]},
                ]
            }
        }
        learnings_file.write_text(json.dumps(learnings_data))

        # Act
        stats = curator.generate_statistics()

        # Assert
        assert stats["by_tag"]["testing"] == 2
        assert stats["by_tag"]["quality"] == 1


class TestAddLearning:
    """Test adding new learnings."""

    def test_add_learning_creates_entry(self, temp_project):
        """Test that add_learning creates new entry."""
        # Arrange
        project_root, curator = temp_project
        learnings_file = project_root / ".session" / "tracking" / "learnings.json"
        learnings_file.parent.mkdir(parents=True)

        import json

        learnings_data = {"categories": {"best_practices": []}}
        learnings_file.write_text(json.dumps(learnings_data))

        # Act
        learning_id = curator.add_learning(
            content="Test learning",
            category="best_practices",
            session=5,
        )

        # Assert
        assert learning_id is not None
        saved_data = json.loads(learnings_file.read_text())
        assert len(saved_data["categories"]["best_practices"]) == 1

    def test_add_learning_with_tags(self, temp_project):
        """Test adding learning with tags."""
        # Arrange
        project_root, curator = temp_project
        learnings_file = project_root / ".session" / "tracking" / "learnings.json"
        learnings_file.parent.mkdir(parents=True)

        import json

        learnings_data = {"categories": {"best_practices": []}}
        learnings_file.write_text(json.dumps(learnings_data))

        # Act
        curator.add_learning(
            content="Test learning",
            category="best_practices",
            tags=["tag1", "tag2"],
        )

        # Assert
        saved_data = json.loads(learnings_file.read_text())
        learning = saved_data["categories"]["best_practices"][0]
        assert "tags" in learning
        assert learning["tags"] == ["tag1", "tag2"]

    def test_add_learning_if_new_adds_unique(self, temp_project):
        """Test that add_learning_if_new adds unique learnings."""
        # Arrange
        project_root, curator = temp_project
        learnings_file = project_root / ".session" / "tracking" / "learnings.json"
        learnings_file.parent.mkdir(parents=True)

        import json

        learnings_data = {"categories": {"best_practices": []}}
        learnings_file.write_text(json.dumps(learnings_data))

        # Act
        result = curator.add_learning_if_new({"content": "New unique learning"})

        # Assert
        assert result is True

    def test_add_learning_if_new_skips_duplicate(self, temp_project):
        """Test that add_learning_if_new skips duplicates."""
        # Arrange
        project_root, curator = temp_project
        learnings_file = project_root / ".session" / "tracking" / "learnings.json"
        learnings_file.parent.mkdir(parents=True)

        import json

        learnings_data = {"categories": {"best_practices": [{"content": "Existing learning"}]}}
        learnings_file.write_text(json.dumps(learnings_data))

        # Act
        result = curator.add_learning_if_new({"content": "Existing learning"})

        # Assert
        assert result is False


class TestCurationConfig:
    """Test curation configuration."""

    def test_load_curation_config_existing_file(self, temp_project):
        """Test loading config from existing file."""
        # Arrange
        project_root, curator = temp_project
        config_file = project_root / ".session" / "config.json"
        config_file.parent.mkdir(parents=True)

        import json

        config_data = {
            "curation": {
                "auto_curate": True,
                "frequency": 3,
            }
        }
        config_file.write_text(json.dumps(config_data))

        # Act
        config = curator._load_curation_config()

        # Assert
        assert config["auto_curate"] is True
        assert config["frequency"] == 3

    def test_load_curation_config_returns_defaults(self, curator):
        """Test loading config returns defaults when file missing."""
        # Act
        config = curator._load_curation_config()

        # Assert
        assert "auto_curate" in config
        assert "frequency" in config


class TestAutoCuration:
    """Test automatic curation."""

    def test_auto_curate_if_needed_disabled(self, temp_project):
        """Test auto-curation when disabled."""
        # Arrange
        project_root, curator = temp_project
        config_file = project_root / ".session" / "config.json"
        config_file.parent.mkdir(parents=True)

        import json

        config_data = {"curation": {"auto_curate_enabled": False}}
        config_file.write_text(json.dumps(config_data))

        # Act
        result = curator.auto_curate_if_needed()

        # Assert
        assert result is False

    def test_auto_curate_if_needed_never_curated(self, temp_project, capsys):
        """Test auto-curation when never curated."""
        # Arrange
        project_root, curator = temp_project
        config_file = project_root / ".session" / "config.json"
        config_file.parent.mkdir(parents=True)

        import json

        config_data = {"curation": {"auto_curate_enabled": True}}
        config_file.write_text(json.dumps(config_data))

        learnings_file = project_root / ".session" / "tracking" / "learnings.json"
        learnings_file.parent.mkdir(parents=True)
        learnings_data = {"categories": {}, "last_curated": None}
        learnings_file.write_text(json.dumps(learnings_data))

        # Act
        result = curator.auto_curate_if_needed()

        # Assert
        assert result is True


class TestReportGeneration:
    """Test report generation."""

    def test_generate_report_displays_categories(self, temp_project, capsys):
        """Test that report displays category counts."""
        # Arrange
        project_root, curator = temp_project
        learnings_file = project_root / ".session" / "tracking" / "learnings.json"
        learnings_file.parent.mkdir(parents=True)

        import json

        learnings_data = {
            "categories": {
                "best_practices": [{"id": "1"}, {"id": "2"}],
                "gotchas": [{"id": "3"}],
            }
        }
        learnings_file.write_text(json.dumps(learnings_data))

        # Act
        curator.generate_report()

        # Assert
        captured = capsys.readouterr()
        assert "Best Practices" in captured.out or "best_practices" in captured.out

    def test_generate_report_shows_total(self, temp_project, capsys):
        """Test that report shows total count."""
        # Arrange
        project_root, curator = temp_project
        learnings_file = project_root / ".session" / "tracking" / "learnings.json"
        learnings_file.parent.mkdir(parents=True)

        import json

        learnings_data = {
            "categories": {
                "best_practices": [{"id": "1"}, {"id": "2"}],
            }
        }
        learnings_file.write_text(json.dumps(learnings_data))

        # Act
        curator.generate_report()

        # Assert
        captured = capsys.readouterr()
        assert "Total" in captured.out or "2" in captured.out


class TestLoadLearningsFile:
    """Test loading learnings from file."""

    def test_load_learnings_existing_file(self, temp_project):
        """Test loading existing learnings file."""
        # Arrange
        project_root, curator = temp_project
        learnings_file = project_root / ".session" / "tracking" / "learnings.json"
        learnings_file.parent.mkdir(parents=True)

        import json

        test_data = {
            "metadata": {"total_learnings": 5},
            "categories": {"best_practices": []},
        }
        learnings_file.write_text(json.dumps(test_data))

        # Act
        result = curator._load_learnings()

        # Assert
        assert result["metadata"]["total_learnings"] == 5
        assert "categories" in result

    def test_load_learnings_creates_default_structure(self, temp_project):
        """Test loading creates default structure when file missing."""
        # Arrange
        project_root, curator = temp_project
        # Don't create the learnings file, so it will create default structure

        # Act
        result = curator._load_learnings()

        # Assert
        assert "metadata" in result
        assert result["metadata"]["total_learnings"] == 0
        assert "categories" in result
        assert "archived" in result


class TestCategorizationWorkflow:
    """Test categorization workflow."""

    @patch.object(LearningsCurator, "_extract_learnings_from_sessions")
    def test_categorize_learnings_adds_new(self, mock_extract, temp_project):
        """Test categorization adds new learnings."""
        # Arrange
        project_root, curator = temp_project
        mock_extract.return_value = [{"content": "Use dependency injection for testability"}]
        learnings = {"categories": {}}

        # Act
        count = curator._categorize_learnings(learnings)

        # Assert
        assert count == 1

    @patch.object(LearningsCurator, "_extract_learnings_from_sessions")
    def test_categorize_learnings_skips_duplicates(self, mock_extract, curator):
        """Test categorization skips duplicates."""
        # Arrange
        mock_extract.return_value = [{"content": "Use dependency injection"}]
        learnings = {"categories": {"best_practices": [{"content": "Use dependency injection"}]}}

        # Act
        count = curator._categorize_learnings(learnings)

        # Assert
        assert count == 0

    def test_extract_learnings_from_sessions_finds_learnings(self, temp_project):
        """Test extraction from session learnings field."""
        # Arrange
        project_root, curator = temp_project
        summaries_dir = project_root / ".session" / "summaries"
        summaries_dir.mkdir(parents=True)
        summary_file = summaries_dir / "session_001.json"

        import json

        summary_data = {
            "learnings": ["Learning one", "Learning two"],
            "timestamp": "2025-10-24T10:00:00",
        }
        summary_file.write_text(json.dumps(summary_data))

        # Act
        result = curator._extract_learnings_from_sessions()

        # Assert
        assert len(result) == 2

    def test_extract_learnings_from_sessions_finds_challenges(self, temp_project):
        """Test extraction from challenges_encountered field."""
        # Arrange
        project_root, curator = temp_project
        summaries_dir = project_root / ".session" / "summaries"
        summaries_dir.mkdir(parents=True)
        summary_file = summaries_dir / "session_001.json"

        import json

        summary_data = {
            "challenges_encountered": ["Challenge one", "Challenge two"],
            "timestamp": "2025-10-24T10:00:00",
        }
        summary_file.write_text(json.dumps(summary_data))

        # Act
        result = curator._extract_learnings_from_sessions()

        # Assert
        assert len(result) == 2

    def test_extract_learnings_from_sessions_handles_invalid_json(self, temp_project):
        """Test that invalid JSON files are skipped gracefully."""
        # Arrange
        project_root, curator = temp_project
        summaries_dir = project_root / ".session" / "summaries"
        summaries_dir.mkdir(parents=True)
        summary_file = summaries_dir / "session_001.json"
        summary_file.write_text("{ invalid json }")

        # Act
        result = curator._extract_learnings_from_sessions()

        # Assert - Should not crash, just return empty list
        assert result == []


class TestAutoCurationFrequency:
    """Tests for auto-curation frequency checking."""

    def test_auto_curate_if_needed_with_frequency_check(self, temp_project):
        """Test auto-curation triggers based on frequency."""
        # Arrange
        project_root, curator = temp_project
        config_dir = project_root / ".session"
        config_dir.mkdir(parents=True)

        import json
        from datetime import datetime, timedelta

        # Create config with auto-curation enabled
        config_file = config_dir / "config.json"
        config_data = {"curation": {"auto_curate_enabled": True, "auto_curate_frequency_days": 7}}
        config_file.write_text(json.dumps(config_data))

        # Create learnings with old curation date
        learnings_file = config_dir / "tracking" / "learnings.json"
        learnings_file.parent.mkdir(parents=True)
        old_date = datetime.now() - timedelta(days=10)
        learnings_data = {
            "categories": {},
            "last_curated": old_date.isoformat(),
            "metadata": {"total_learnings": 0},
        }
        learnings_file.write_text(json.dumps(learnings_data))

        # Act
        from unittest.mock import patch

        with patch.object(curator, "curate") as mock_curate:
            result = curator.auto_curate_if_needed()

        # Assert
        assert result is True
        mock_curate.assert_called_once_with(dry_run=False)

    def test_auto_curate_if_needed_skips_when_recent(self, temp_project):
        """Test auto-curation skips when recently curated."""
        # Arrange
        project_root, curator = temp_project
        config_dir = project_root / ".session"
        config_dir.mkdir(parents=True)

        import json
        from datetime import datetime, timedelta

        # Create config with auto-curation enabled
        config_file = config_dir / "config.json"
        config_data = {"curation": {"auto_curate_enabled": True, "auto_curate_frequency_days": 7}}
        config_file.write_text(json.dumps(config_data))

        # Create learnings with recent curation date
        learnings_file = config_dir / "tracking" / "learnings.json"
        learnings_file.parent.mkdir(parents=True)
        recent_date = datetime.now() - timedelta(days=2)
        learnings_data = {
            "categories": {},
            "last_curated": recent_date.isoformat(),
            "metadata": {"total_learnings": 0},
        }
        learnings_file.write_text(json.dumps(learnings_data))

        # Act
        result = curator.auto_curate_if_needed()

        # Assert
        assert result is False


class TestCurationConfigExceptionHandling:
    """Tests for curation configuration loading."""

    def test_load_curation_config_handles_exception(self, temp_project):
        """Test that config loading handles exceptions gracefully."""
        # Arrange
        project_root, curator = temp_project
        config_dir = project_root / ".session"
        config_dir.mkdir(parents=True)

        # Create invalid config file
        config_file = config_dir / "config.json"
        config_file.write_text("{ invalid json }")

        # Act
        config = curator._load_curation_config()

        # Assert - Should return defaults
        assert "auto_curate" in config or "similarity_threshold" in config


class TestSessionSummaryExtraction:
    """Tests for extracting learnings from session summary files."""

    def test_extract_from_session_summary_file_not_exists(self, temp_project):
        """Test extraction handles missing file."""
        # Arrange
        project_root, curator = temp_project
        non_existent = project_root / "nonexistent.md"

        # Act
        result = curator.extract_from_session_summary(non_existent)

        # Assert
        assert result == []

    def test_extract_from_session_summary_file_read_error(self, temp_project):
        """Test extraction handles file read errors."""
        # Arrange
        project_root, curator = temp_project
        summaries_dir = project_root / ".session" / "summaries"
        summaries_dir.mkdir(parents=True)
        summary_file = summaries_dir / "session_005_summary.md"
        summary_file.write_text("## Learnings Captured\n- Test learning")

        # Act - Mock file read to raise exception
        from unittest.mock import patch

        with patch("builtins.open", side_effect=Exception("Read error")):
            result = curator.extract_from_session_summary(summary_file)

        # Assert
        assert result == []

    def test_extract_from_session_summary_with_challenges(self, temp_project):
        """Test extraction from Challenges Encountered section."""
        # Arrange
        project_root, curator = temp_project
        summaries_dir = project_root / ".session" / "summaries"
        summaries_dir.mkdir(parents=True)
        summary_file = summaries_dir / "session_007_summary.md"

        content = """# Session Summary

## Challenges Encountered
- Complex debugging issue with async code
- Performance bottleneck in database queries

## Other Section
- Not a learning
"""
        summary_file.write_text(content)

        # Act
        result = curator.extract_from_session_summary(summary_file)

        # Assert
        assert len(result) >= 1


class TestCodeCommentExtraction:
    """Tests for extracting learnings from code comments."""

    def test_extract_from_code_comments_handles_git_error(self, temp_project):
        """Test extraction handles git command failures gracefully."""
        # Arrange
        project_root, curator = temp_project

        # Act - In non-git directory, should handle gracefully
        import subprocess
        from unittest.mock import patch

        with patch("subprocess.run", side_effect=subprocess.CalledProcessError(1, "git")):
            result = curator.extract_from_code_comments(session_id="session_001")

        # Assert - Should not crash
        assert isinstance(result, list)


class TestSessionNumberExtractionEdgeCases:
    """Tests for session number extraction edge cases."""

    def test_extract_session_number_handles_exception(self, temp_project):
        """Test session number extraction handles invalid input."""
        # Arrange
        project_root, curator = temp_project

        # Act
        result = curator._extract_session_number("invalid_format_###")

        # Assert
        assert result == 0


class TestCurateDryRun:
    """Tests for dry run functionality."""

    def test_curate_dry_run_prints_message(self, temp_project, capsys):
        """Test that dry run mode prints appropriate message."""
        # Arrange
        project_root, curator = temp_project
        learnings_file = project_root / ".session" / "tracking" / "learnings.json"
        learnings_file.parent.mkdir(parents=True)

        import json

        learnings_data = {"categories": {}, "metadata": {"total_learnings": 0}}
        learnings_file.write_text(json.dumps(learnings_data))

        # Act
        curator.curate(dry_run=True)
        captured = capsys.readouterr()

        # Assert
        assert "Dry run" in captured.out or "dry run" in captured.out.lower()
